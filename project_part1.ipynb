{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Project Part 1\n","\n","[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sgeinitz/CS39AA-project/blob/main/project_part1.ipynb)\n","\n","This notebook is intended to serve as a template to complete Part 1 of the projects. Feel free to modify this notebook as needed, but be sure to have the two main parts, a) a introductory proposal section describing what it is your doing to do and where the dataset originates, and b) an exploratory analysis section that has the histograms, charts, tables, etc. that are the output from your exploratory analysis. \n","\n","__Note you will want to remove the text above, and in the markdown cells below, and replace it with your own text describing the dataset, task, exploratory steps, etc.__"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introduction/Background\n","\n","_In this section you will describe (in English) the dataset you are using as well as the NLP problem it deals with. For example, if you are planning to use the Twitter Natural Disaster dataset, then you will describe what the data and where it came as if you were explaining it to someone who does not know anything about the data. You will then describe how this is a __text classification__ problem, and that the labels are binary (e.g. a tweet either refers to a genuine/real natural disaster, or it does not)._ \n","\n","_Overall, this should be about a paragraph of text that could be read by someone outside of our class, and they could still understand what it is your project is doing._ \n","\n","_Note that you should __not__ simply write one sentence stating, \"This project is base on the Kaggle competition: Predicting Natural Disasters with Twitter._\"\n","\n","_If you are still looking for datasets to use, consider the following resources to explore text datasets._\n","\n","* https://huggingface.co/datasets/\n","* https://www.kaggle.com/datasets\n","* https://data-flair.training/blogs/machine-learning-datasets/ \n","* https://pytorch.org/text/stable/datasets.html\n","* https://github.com/niderhoff/nlp-datasets \n","* https://medium.com/@ODSC/20-open-datasets-for-natural-language-processing-538fbfaf8e38 \n","* https://imerit.net/blog/25-best-nlp-datasets-for-machine-learning-all-pbm/ \n","\n","\n","_If you instead are planning to do a more research-oriented or applied type of project, then describe what it is that you plan to do._\n","\n","_If it is research, then what do you want to understand/explain better?_\n","\n","_If it is applied, then what it is you plan to build?_ "]},{"cell_type":"markdown","metadata":{},"source":["The data set that I am using for my NLP project is from kaggle. The data set involves a given text and the emotions that is related to that text or predicted emotion for the text. That is what I am trying to do for my project which is to determine the type of emotion that is associated with the text or predict what the emotion is based on a text that is given. This is a text classification because it classify or puts the text in a group depending on what emotion the text is giving off. It is binary because it is going to be associted to only one emotion that the text is trying to convey. Even though at times text can display more than one emotions, there is always one emotion that can be displayed more off a text. The input for this project would be a any given text and the output would be what type of emotion the text is classified as. "]},{"cell_type":"markdown","metadata":{},"source":["## 2. Exploratory Data Analysis\n","\n","_You will now load the dataset and carry out some exploratory data analysis steps to better understand what text data looks like. See the examples from class on 10/. The following links provide some good resources of exploratory analyses of text data with Python._\n","\n","\n","* https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools\n","* https://regenerativetoday.com/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis/\n","* https://medium.com/swlh/text-summarization-guide-exploratory-data-analysis-on-text-data-4e22ce2dd6ad  \n","* https://www.kdnuggets.com/2019/05/complete-exploratory-data-analysis-visualization-text-data.html  \n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'numpy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This Python 3 environment comes with many helpful analytics libraries installed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# For example, here's several helpful packages to load\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \u001b[38;5;66;03m# linear algebra\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \u001b[38;5;66;03m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Input data files are available in the read-only \"../input/\" directory\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/emotion/Emotion_final.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['Text'].str.len().hist()"]},{"cell_type":"markdown","metadata":{},"source":["This is showing how many charcters there are in the text that I could possiby break it down to and narrow down. Based on this data best possible way is to trim the characters to less than 50. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_word_number_histogram(text):\n","    text.str.split().\\\n","        map(lambda x: len(x)).\\\n","        hist()\n","\n","plot_word_number_histogram(data['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["The histrogram is showing how many words are in the text. Ranges from what it looks to be 5 to I want to say 75. Would need to cut it down to around 20 since most text looks to be on average of that."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_word_length_histogram(text):\n","    text.str.split().\\\n","        apply(lambda x : [len(i) for i in x]). \\\n","        map(lambda x: np.mean(x)).\\\n","        hist()\n","\n","plot_word_length_histogram(data['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["In the histogram it is showing what are the average word length in the text. This is showing that it is in between aaround 2.7 to 6.8. The average and good amount would be around 4. Might want to do more than that to more accuracy on what the emotions would come out to. just because 4 seems to be too little to just predict emotions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from nltk.corpus import stopwords\n","\n","def plot_top_stopwords_barchart(text):\n","    stop=set(stopwords.words('english'))\n","    \n","    new= text.str.split()\n","    new=new.values.tolist()\n","    corpus=[word for i in new for word in i]\n","    from collections import defaultdict\n","    dic=defaultdict(int)\n","    for word in corpus:\n","        if word in stop:\n","            dic[word]+=1\n","            \n","    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n","    x,y=zip(*top)\n","    plt.bar(x,y)\n","    \n","plot_top_stopwords_barchart(data['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["Wanted to filter out the stop words that could possibly cause the data to be putting the output incorrectly or take it out so the output could be more correct. It seems that I is the most occurent stop word that needs to be taken out of the text to get more of an accurate read. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","from nltk.corpus import stopwords\n","from collections import  Counter\n","\n","def plot_top_non_stopwords_barchart(text):\n","    stop=set(stopwords.words('english'))\n","    \n","    new= text.str.split()\n","    new=new.values.tolist()\n","    corpus=[word for i in new for word in i]\n","\n","    counter=Counter(corpus)\n","    most=counter.most_common()\n","    x, y=[], []\n","    for word,count in most[:40]:\n","        if (word not in stop):\n","            x.append(word)\n","            y.append(count)\n","            \n","    sns.barplot(x=y,y=x)\n","    \n","plot_top_non_stopwords_barchart(data['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["Here in he historgram wanted to see what other words would appear that has came up frequently. I could use these words as to target what the emotion could be once these words pop up. After these words pop up I could see what comes next that could determine what the emotions of text could come out to be."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","import numpy as np\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from collections import  Counter\n","\n","def plot_top_ngrams_barchart(text, n=2):\n","    stop=set(stopwords.words('english'))\n","\n","    new= text.str.split()\n","    new=new.values.tolist()\n","    corpus=[word for i in new for word in i]\n","\n","    def _get_top_ngram(corpus, n=None):\n","        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n","        bag_of_words = vec.transform(corpus)\n","        sum_words = bag_of_words.sum(axis=0) \n","        words_freq = [(word, sum_words[0, idx]) \n","                      for word, idx in vec.vocabulary_.items()]\n","        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","        return words_freq[:10]\n","\n","    top_n_bigrams=_get_top_ngram(text,n)[:10]\n","    x,y=map(list,zip(*top_n_bigrams))\n","    sns.barplot(x=y,y=x)\n","    \n","plot_top_ngrams_barchart(data['Text'],2)"]},{"cell_type":"markdown","metadata":{},"source":["Used this to determine what two words go hand and hand together that could deteremine and help predict an emotion. The one that came out the most was feel like. Which would make sense since anytime someone were to express emoitons they would say something along the lines of \"feel like.\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_top_ngrams_barchart(data['Text'],3)"]},{"cell_type":"markdown","metadata":{},"source":["I did the same thing with this histogram, but this time with three words coming together most often. Three words that are linked together seems to be feel like im. Might used this more because ususally a person would say these three words when expressing themselves. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","    \n","def plot_polarity_histogram(text):\n","    \n","    def _polarity(text):\n","        return TextBlob(text).sentiment.polarity\n","        \n","    polarity_score =text.apply(lambda x : _polarity(x))\n","    polarity_score.hist()\n","    \n","plot_polarity_histogram(data['Text'])"]},{"cell_type":"markdown","metadata":{},"source":["Using polarity to see if emotions of text is either more happy or negative. Based on the histogram emotions are neutral. It's not one sided to see if it is postive emotions or negative. It's balanced which is good so know that it's not to skwed of a data. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from textblob import TextBlob\n","import matplotlib.pyplot as plt\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","\n","def sentiment_vader(text, sid):\n","    ss = sid.polarity_scores(text)\n","    ss.pop('compound')\n","    return max(ss, key=ss.get)\n","\n","def sentiment_textblob(text):\n","        x = TextBlob(text).sentiment.polarity\n","        \n","        if x<0:\n","            return 'neg'\n","        elif x==0:\n","            return 'neu'\n","        else:\n","            return 'pos'\n","\n","def plot_sentiment_barchart(text, method='TextBlob'):\n","    if method == 'TextBlob':\n","        sentiment = text.map(lambda x: sentiment_textblob(x))\n","    elif method == 'Vader':\n","        nltk.download('vader_lexicon')\n","        sid = SentimentIntensityAnalyzer()\n","        sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))\n","    else:\n","        raise ValueError('Textblob or Vader')\n","    \n","    plt.bar(sentiment.value_counts().index,\n","            sentiment.value_counts())\n","    \n","plot_sentiment_barchart(data['Text'], method='Vader')"]},{"cell_type":"markdown","metadata":{},"source":["Wantd to see more if the text that was given was skwed to be more negative or positive emotions. From the histogram it looks like it is not one sided. Which is good so it is not biased data. "]},{"cell_type":"markdown","metadata":{},"source":["As a conclusion I could use the legnth of the text to help trim down the data so a model could determine emotions. Take out unnesscary stop words that do not need to be in text to help the model to get output. I could use three or two words that are usaully together to determine emotion. I. could take out unnesscary charcters as well. Based on historgrams this data seems to be neutral in determining emotions. It's not one side so would be good to use for this project. "]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
